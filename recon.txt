from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bigquery_operator import BigQueryOperator
from airflow.operators.gcs_operator import GoogleCloudStorageCreateBucketOperator, \
    GoogleCloudStorageUploadOperator
from airflow.operators.dummy_operator import DummyOperator

default_args = {
    'owner': 'your_owner',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'reconciliation_dag',
    default_args=default_args,
    description='DAG for reconciliation and export to GCS',
    schedule_interval='@daily',  # You can adjust the schedule as needed
)

# Define BigQuery SQL query for reconciliation
reconciliation_query = """
SELECT
  column1,
  column2,
  -- Add your reconciliation logic here
FROM
  your_dataset.your_table
"""

# BigQuery operator for reconciliation
reconciliation_task = BigQueryOperator(
    task_id='reconciliation_query',
    sql=reconciliation_query,
    destination_dataset_table='your_dataset.reconciliation_result',
    write_disposition='WRITE_TRUNCATE',  # Overwrite the destination table
    dag=dag,
)

# GCS bucket and object details
gcs_bucket_name = 'your_gcs_bucket'
gcs_object_name = 'your_folder/reconciliation_result.xlsx'

# GCS bucket creation operator
create_bucket_task = GoogleCloudStorageCreateBucketOperator(
    task_id='create_gcs_bucket',
    bucket_name=gcs_bucket_name,
    dag=dag,
)

# GCS upload operator
upload_to_gcs_task = GoogleCloudStorageUploadOperator(
    task_id='upload_to_gcs',
    src='gs://your_dataset/reconciliation_result.xlsx',  # Source file in GCS
    dst=gcs_object_name,
    bucket=gcs_bucket_name,
    dag=dag,
)

# Dummy task for indicating the end of the DAG
end_task = DummyOperator(task_id='end', dag=dag)

# Define the task dependencies
reconciliation_task >> create_bucket_task >> upload_to_gcs_task >> end_task
